# Turning Our Single-Threaded Server into a Multithreaded Server

## Simulating a Slow Request in the Current Server Implementation

We'll look at how a slow-processing request can affect other requests made to our current server
implementation. The following implements handling a request to */sleep* with a simulated slow
response that will cause the server to sleep for 5 seconds before responding.

```rust
use std::{
    fs,
    io::{prelude::*, BufReader},
    net::{TcpListener, TcpStream},
    thread,
    time::Duration,
};
// --snip--

fn handle_connection(mut stream: TcpStream) {
    // --snip--

    let (status_line, filename) = match &request_line[..] {
        "GET / HTTP/1.1" => ("HTTP/1.1 200 OK", "hello.html"),
        "GET /sleep HTTP/1.1" => {
            thread::sleep(Duration::from_secs(5));
            ("HTTP/1.1 200 OK", "hello.html")
        }
        _ => ("HTTP/1.1 404 NOT FOUND", "404.html"),
    };

    // --snip--
}
```

We switched from `if` to `match` now that we have three cases. We need to explicitly match on a
slice of `request_line` to pattern match against the string literal values; `match` doesn't do
automatic referencing and dereferencing like the equality method does.

The first arm matches to */*, the second arm matches to */sleep*, and the third arm is a catch-all
for the error page. When /sleep is requested, the server will sleep for 5 seconds before rendering
the successful HTML page.

## Improving Throughput with a Thread Pool

A *thread pool* is a group of spawned threads that're waiting and ready to handle a task. When the
program receives a new task, it assigns one of the threads in the pool to the task, and that thread
will process the task. The remaining threads in the pool are available to handle any other tasks
that come in while the first thread is processing. When the first thread is done processing its
task, it's returned to the pool of idle threads, ready to handle a new task. A thread pool allows
you to process connections concurrently, increasing the throughput of your server.

We'll limit the number of threads in the pool to a small number to protect us from Denial of Service
(DoS) attacks; if we had our program create a new thread for each request as it came in, someone
making 10 million requests to our server could create havoc by using up all our server's resources
and grinding the processing of requests to a halt.

We'll have a fixed number of threads waiting in the pool. Requests that come in are sent to the pool
for processing. The pool will maintain a queue of incoming requests. Each of the threads in the pool
will pop off a request from this queue, handle the request, and then ask the queue for another
request. With this design, we can process up to `N` requests concurrently, where `N` is the number
of threads. If each thread is responding to a long-running request, subsequent requests can still
back up in the queue, but we've increased the number of long-running requests we can handle before
reaching that point.

This technique is just one of many ways to improve the throughput of a web server. Other options you
might explore are the *fork/join model*, the *single-threaded async I/O model*, or the
*multi-threaded async I/O model*. If you're interested in this topic, you can read more about other
solutions and try to implement them; with a low-level language like Rust, all of these options are
possible.

When you're trying to design code, writing the client interface first can help guide your design.
Write the API of the code so it's structured in the way you want to call it; then implement the
functionality within that structure rather than implementing the functionality and then designing
the public API.

Similar to test-driven development used in the Chapter 12 project, we'll use compiler-driven
development here. We'll write the code that calls the functions we want, and then we'll look at
errors from the compiler to determine what we should change next to get the code to work. But before
that, we'll explore the technique we're *not* going to use as a starting point.

### Spawning a Thread for Each Request

The following shows the changes to make to `main` to spawn a new thread to handle each stream within
the `for` loop:

```rust
fn main() {
    let listener = TcpListener::bind("127.0.0.1:7878").unwrap();

    for stream in listener.incoming() {
        let stream = stream.unwrap();

        thread::spawn(|| {
            handle_connection(stream);
        });
    }
}
```

### Creating a Finite Number of Threads

We want our thread pool to work in a similar, familiar way so switching from threads to a thread
pool doesn't require large changes to the code that uses our API. The following shows the
hypothetical interface for a `ThreadPool` struct we want to use instead of `thread::spawn` (doesn't
compile yet).

```rust
fn main() {
    let listener = TcpListener::bind("127.0.0.1:7878").unwrap();
    let pool = ThreadPool::new(4);

    for stream in listener.incoming() {
        let stream = stream.unwrap();

        pool.execute(|| {
            handle_connection(stream);
        });
    }
}
```

We use `ThreadPool::new` to create a new thread pool with a configurable number of threads. Then, in
the `for` loop, `pool.execute` has a similar interface as `thread::spawn` in that it takes a closure
the pool should run for each stream. We need to implement `pool.execute` so it takes the closure and
gives it to a thread in the pool to run.

### Building ThreadPool Using Compiler Driven Development

After changing `main`, the first error we get tells us we need to make `ThreadPool`.

Switch the `hello` crate from a binary crate to a library crate to hold our `ThreadPool`
implementation. After we change to a library crate, we could also use the separate thread pool
library for any work we want to do using a thread pool, not just for serving web requests.

In *lib.rs*, define `ThreadPool` and add associated function `new`:

```rust
pub struct ThreadPool;

impl ThreadPool {
    pub fn new(size: usize) -> ThreadPool {
        ThreadPool
    }
}
```

Now we'll get an error b/c we don't have an `execute` method on `ThreadPool`. Recall that we decided
our thread pool should have an interface similar to `thread::spawn`. In addition, we'll implement
the `execute` function so it takes the closure it's given and gives it to an idle thread in the pool
to run.

Define `execute` on `ThreadPool` to take a closure as a parameter. Recall that we can take closures
as parameters with three different traits: Fn, FnMut, and FnOnce. We need to decide which kind of
closure to use here. We know we'll end up doing something similar to the standard library
`thread::spawn` implementation, so we can look at what bounds the signature of `thread::spawn` has
on its parameter. The documentation shows us the following:

```rust
pub fn spawn<F, T>(f: F) -> JoinHandle<T>
    where
        F: FnOnce() -> T,
        F: Send + 'static,
        T: Send + 'static,
```

So now do something similar to `ThreadPool::execute`, but only pay attention to the `F` generic b/c
`execute` won't be returning anything.

```rust
    pub fn execute<F>(&self, f: F)
    where
        F: FnOnce() + Send + 'static
    {
        f()
    }
```

### Validating the Number of Threads in `new`

We aren't doing anything with the parameters to `new` and `execute`. Let's implement the bodies of
these functions with the behavior we want. To start, let's think about `new`. Earlier we chose an
unsigned type for the `size` parameter, because a pool with a negative number of threads makes no
sense. However, a pool with zero threads also makes no sense, yet zero is a perfectly valid usize.
We'll add code to check that size is greater than zero before we return a `ThreadPool` instance and
have the program panic if it receives a zero by using `assert!`:

```rust
    /// Create a new `ThreadPool`.
    /// 
    /// `size` is the number of threads in the pool.
    /// 
    /// # Panics
    /// 
    /// The `new` function will panic if `size` is zero.
    pub fn new(size: usize) -> ThreadPool {
        assert!(size > 0);

        ThreadPool
    }
```

### Creating Space to Store the Threads

Now we can create those threads and store them in the `ThreadPool` struct before returning the
struct. But how do we "store" a thread? Take another look at the `thread::spawn` signature:

`spawn` returns a `JoinHandle<T>`, where `T` is the type that the closure returns. Let's try using
`JoinHandle` too and see what happens. In our case, the closures we're passing to the thread pool
will handle the connection and not return anything, so `T` will be `()`.

The following code will compile but doesn't create any threads yet. We've changed the definition of
`ThreadPool` to hold a vector of `thread::JoinHandle<()>` instances, initialized the vector with a
capacity of `size`, set up a `for` loop that'll run some code to create the threads, and returned a
`ThreadPool` instance containing them:

```rust
use std::thread;

pub struct ThreadPool {
    threads: Vec<thread::JoinHandle<()>>,
}

impl ThreadPool {
    // --snip--
    pub fn new(size: usize) -> ThreadPool {
        assert!(size > 0);

        let mut threads = Vec::with_capacity(size);

        for _ in 0..size {
            // create some threads and store them in the vector
        }

        ThreadPool { threads }
    }
    // --snip--
}
```

Brought `std::thread` into scope in the library crate, b/c we're using `thread::JoinHandle` as the
type of the items in the vector in `ThreadPool`.

Once a valid size is received, `ThreadPool` creates a new vector that can hold `size` items. The
`with_capacity` function performs the same task as `Vec::new` but with an important difference: it
preallocates space in the vector. Because we know we need to store `size` elements in the vector,
doing this allocation up front is slightly more efficient than using `Vec::new`, which resizes
itself as elements are inserted. Running `cargo check` again should succeed.

### A `Worker` Struct Responsible for Sending Code from the `ThreadPool` to a Thread

Here, we'll look at how we actually create threads. The stdlib provides `thread::spawn` as a way to
create threads, and `thread::spawn` expects to get some code the thread should run as soon as the
thread is created. However, we want to create the threads and have them wait for code that we'll
send later. The stdlib's implementation of threads doesn't include any way to do that (have to
implement manually).

We'll implement this behavior by introducing a new data structure between the `ThreadPool` and the
threads that will manage this new behavior. We'll call this data structure `Worker`, which is a
common term in pooling implementations. The `Worker` picks up code that needs to be run and runs the
code in the `Worker`'s thread.

Instead of storing a vector of `JoinHandle<()>` instances in the thread pool, we'll store instances
of the `Worker` struct. Each `Worker` will store a single `JoinHandle<()>` instance. Then we'll
implement a method on `Worker` that will take a closure of code to run and send it to the already
running thread for execution. We'll also give each worker an `id` so we can distinguish between the
different workers in the pool when logging or debugging.

Here's the new process that'll happen when we create a `ThreadPool`. We'll implement the code that
sends the closure to the thread after we have `Worker` set up in this way:

1. Define a `Worker` struct that holds an `id` and a `JoinHandle<()>`.
2. Change `ThreadPool` to hold a vector of `Worker` instances.
3. Define a `Worker::new` function that takes an `id` number and returns a `Worker` instance that
   holds the `id` and a thread spawned with an empty closure.
4. In `ThreadPool::new`, use the `for` loop counter to generate an `id`, create a new `Worker` with
   that `id`, and store the worker in the vector.

```rust
use std::thread;

pub struct ThreadPool {
    workers: Vec<Worker>,
}

impl ThreadPool {
    // --snip--
    pub fn new(size: usize) -> ThreadPool {
        assert!(size > 0);

        let mut workers = Vec::with_capacity(size);

        for id in 0..size {
            workers.push(Worker::new(id));
        }

        ThreadPool { workers }
    }
    // --snip--
}

struct Worker {
    id: usize,
    thread: thread::JoinHandle<()>,
}

impl Worker {
    fn new(id: usize) -> Worker {
        let thread = thread::spawn(|| {});

        Worker { id, thread }
    }
}
```

> Note: If the OS can't create a thread because there aren't enough system resources,
> `thread::spawn` will panic. That'll cause the whole server to panic, even though the creation of
> some threads might succeed. For simplicity, this behavior is fine, but in a production thread pool
> implementation, you'd likely want to use `std::thread::Builder` and its `spawn` method that
> returns `Result` instead.

This code will compile and will store the number of `Worker` instances we specified as an argument
to `ThreadPool::new`. But we're still not processing the closure that we get in `execute`.

### Sending Requests to Threads via Channels

The next thing to tackle is that the closures given to `thread::spawn` do absolutely nothing.
Currently, we get the closure we want to execute in the `execute` method. But we need to give
`thread::spawn` a closure to run when we create each `Worker` during the creation of the
`ThreadPool`.

We want each `Worker` struct that we just created to fetch the code to run from a queue held in the
`ThreadPool` and send that code to the worker's thread to run.

The channels we learned about in Chapter 16 would be perfect for this use case. We'll use a channel
to function as the queue of jobs, and `execute` will send a job from the `ThreadPool` to the
`Worker` instances, which will send the job to its thread. Here is the plan:

1. The `ThreadPool` will create a channel and hold on to the sender.
2. Each `Worker` will hold on to the receiver.
3. We'll create a new `Job` struct that'll hold the closures we want to send down the channel.
4. The `execute` method will send the job it wants to execute through the sender.
5. In its thread, the `Worker` will loop over its receiver and execute the closures of any jobs it
   receives.

Start by creating a channel in `ThreadPool::new` and holding the sender in the `ThreadPool`
instance. The `Job` struct doesn't hold anything for now but will be the type of item we're sending
down the channel:

```rust
use std::{sync::mpsc, thread};

pub struct ThreadPool {
    workers: Vec<Worker>,
    sender: mpsc::Sender<Job>,
}

struct Job;

impl ThreadPool {
    // --snip--
    pub fn new(size: usize) -> ThreadPool {
        assert!(size > 0);

        let (sender, receiver) = mpsc::channel();

        let mut workers = Vec::with_capacity(size);

        for id in 0..size {
            workers.push(Worker::new(id));
        }

        ThreadPool { workers, sender }
    }
    // --snip--
}
```

Now try passing a receiver of the channel into each worker as the thread pool creates the worker. We
want to use the receiver in the thread that the workers spawn, so we'll reference the `receiver`
parameter in the closure (the code won't compile yet):

```rust
impl ThreadPool {
    // --snip--
    pub fn new(size: usize) -> ThreadPool {
        assert!(size > 0);

        let (sender, receiver) = mpsc::channel();

        let mut workers = Vec::with_capacity(size);

        for id in 0..size {
            workers.push(Worker::new(id, receiver));
        }

        ThreadPool { workers, sender }
    }
    // --snip--
}

// --snip--

impl Worker {
    fn new(id: usize, receiver: mpsc::Receiver<Job>) -> Worker {
        let thread = thread::spawn(|| {
            receiver;
        });

        Worker { id, thread }
    }
}
```

Here's the error we get:

```
$ cargo check
    Checking hello v0.1.0 (file:///projects/hello)
error[E0382]: use of moved value: `receiver`
  --> src/lib.rs:26:42
   |
21 |         let (sender, receiver) = mpsc::channel();
   |                      -------- move occurs because `receiver` has type `std::sync::mpsc::Receiver<Job>`, which does not implement the `Copy` trait
...
25 |         for id in 0..size {
   |         ----------------- inside of this loop
26 |             workers.push(Worker::new(id, receiver));
   |                                          ^^^^^^^^ value moved here, in previous iteration of loop
   |
note: consider changing this parameter type in method `new` to borrow instead if owning the value isn't necessary
  --> src/lib.rs:47:33
   |
47 |     fn new(id: usize, receiver: mpsc::Receiver<Job>) -> Worker {
   |        --- in this method       ^^^^^^^^^^^^^^^^^^^ this parameter takes ownership of the value
help: consider moving the expression out of the loop so it is only moved once
   |
25 ~         let mut value = Worker::new(id, receiver);
26 ~         for id in 0..size {
27 ~             workers.push(value);
   |

For more information about this error, try `rustc --explain E0382`.
error: could not compile `hello` (lib) due to 1 previous error
```

It's trying to pass `receiver` to multiple `Worker` instances. This won't work: the channel
implementation that Rust provides is *multiple producer, single consumer*. This means we can't just
clone the consuming end of the channel to fix this code. We also don't want to send a message
multiple times to multiple consumers; we want one list of messages with multiple workers such that
each message gets processed once.

Additionally, taking a job off the channel queue involves mutating the `receiver`, so the threads
need a safe way to share and modify `receiver`. Otherwise, we might get race conditions.

Recall thread-safe smart pointers: to share ownership across multiple threads and allow them to
mutate the value, we need to use `Arc<Mutex<T>>`. The `Arc` type will let multiple workers own the
receiver, and `Mutex` will ensure that only one worker gets a job from the receiver at a time:

```rust
use std::{
    sync::{mpsc, Arc, Mutex},
    thread,
};
// --snip--

impl ThreadPool {
    // --snip--
    pub fn new(size: usize) -> ThreadPool {
        assert!(size > 0);

        let (sender, receiver) = mpsc::channel();

        let receiver = Arc::new(Mutex::new(receiver));

        let mut workers = Vec::with_capacity(size);

        for id in 0..size {
            workers.push(Worker::new(id, Arc::clone(&receiver)));
        }

        ThreadPool { workers, sender }
    }

    // --snip--
}

// --snip--

impl Worker {
    fn new(id: usize, receiver: Arc<Mutex<mpsc::Receiver<Job>>>) -> Worker {
        // --snip--
    }
}
```

### Implementing the `execute` Method

The previous implementation of `execute` only works for single-threaded cases. To support
multi-threadedness, we need to wrap `f` in a `Box` and send it down the channel so that workers can
handle the execution. We'll also change `Job` from a struct to a type alias representing the type
that `execute` receives:

```rust
// --snip--

type Job = Box<dyn FnOnce() + Send + 'static>;

// --snip--

impl ThreadPool {
    // --snip--

    pub fn execute<F>(&self, f: F)
    where
        F: FnOnce() + Send + 'static
    {
        let job = Box::new(f);

        self.sender.send(job).unwrap();
    }
}

// --snip--
```

In `Worker`'s `new` method, the closure being passed to `thread::spawn` still only references the
receiving end of the channel. Instead, the closure needs to loop forever, asking the receiving end
of the channel for a job and running the job when it gets one:

```rust
// --snip--

impl Worker {
    fn new(id: usize, receiver: Arc<Mutex<mpsc::Receiver<Job>>>) -> Worker {
        let thread = thread::spawn(move || loop {
            let job = receiver.lock().unwrap().recv().unwrap();

            println!("Worker {id} got a job; executing.");

            job();
        });

        Worker { id, thread }
    }
}
```

`lock` is called on `receiver` to acquire the mutex, and then we call `unwrap` to panic on any
errors. Acquiring a lock might fail if the mutex is in a *poisoned* state, which can happen if some
other thread panicked while holding the lock rather than releasing the lock. In this case, calling
`unwrap` to have this thread panic is the correct action to take.

If we get the lock on the mutex, we call `recv` to receive a `Job` from the channel. A final
`unwrap` moves past any errors here as well, which might occur if the thread holding the sender has
shut down.

The call to `recv` blocks (as in blocking code), so if there's no job yet the current thread will
wait until a job becomes available. The `Mutex<T>` ensures that only one `Worker` thread at a time
is trying to request a job.

We now have a thread pool that executes connections asynchronously. There are never more than four
threads created, so our system can't be DoS attacked. If we make a request to */sleep*, the server
will be able to serve other requests by having another thread run them.

## Extra

It seems like we could've used `while let` to accomplish the previous code in `Worker::new`:

```rust
// --snip--

impl Worker {
    fn new(id: usize, receiver: Arc<Mutex<mpsc::Receiver<Job>>>) -> Worker {
        let thread = thread::spawn(move || {
            while let Ok(job) = receiver.lock().unwrap().recv() {
                println!("Worker {id} got a job; executing.");

                job();
            }
        });

        Worker { id, thread }
    }
}
```

But this code would actually end up with the wrong threading behavior: a slow request will still
cause other requests to wait to be processed. The reason is kinda subtle: the `Mutex` struct has no
public `unlock` method b/c the ownership of the lock is based on the lifetime of the `MutexGuard<T>`
w/in the `LockResult<MutexGuard<T>>` that `lock` returns. At compile time, the borrow checker can
then enforce the rule that a resource guarded by a `Mutex` cannot be accessed unless we hold the
lock. However, this implementation can also result in the lock being held longer than intended if we
aren't mindful of the lifetime of the `MutexGuard<T>`.

The code that uses `let job = receiver.lock().unwrap().recv().unwrap();` works b/c with `let`, any
temporary values used in the expression on the right hand side of the `=` are immediately dropped
when the `let` statement ends. However, `while let` (and `if let` and `match`) don't drop temporary
values until the end of the associated block. In the `while let` code, the lock remains held for the
duration of the call to `job()`, meaning other workers cannot receive jobs.
